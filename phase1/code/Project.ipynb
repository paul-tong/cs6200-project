{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Zhan\n",
      "[nltk_data]     Kefei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue, Empty\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import operator\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = os.getcwd().replace(\"phase1\",\"general\")+\"\\\\test-collection\\\\common_words\" #load stopwords\n",
    "f = open(stops, 'r',encoding='utf-8')\n",
    "stopwords = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Collection = [] # load documents and do parsing job\n",
    "Doc_name = []\n",
    "folder_d = os.getcwd().replace(\"phase1\",\"general\")+\"\\\\test-collection\\\\cacm\"\n",
    "for filename in os.listdir(folder_d):\n",
    "    Doc_name.append(filename.replace(\".html\",\"\"))\n",
    "    file_d = folder_d + \"\\\\\" + filename\n",
    "    f = open(file_d, 'r',encoding='utf-8')\n",
    "    remove = string.punctuation # remove punctuations\n",
    "    pattern = r\"[{}]\".format(remove)\n",
    "    content = f.read().replace(\"\\n\",\" \").replace(\"\\t\",\" \") \n",
    "    content = BeautifulSoup(content, \"html5lib\").get_text()\n",
    "    content = re.sub(pattern, \" \", content).lower() \n",
    "    content = content.replace(\" cacm\" + content.split(\"cacm\")[-1],\"\")  \n",
    "    #ignore the digits that commonly appear in the end of the documentsâ€™ content.\n",
    "    Collection.append(re.sub(r'\\s+', ' ', content).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_query = os.getcwd().replace(\"phase1\",\"general\")+\"\\\\test-collection\\\\cacm.query.txt\" #load query list\n",
    "f = open(file_query, 'r',encoding='utf-8')\n",
    "remove = string.punctuation # remove punctuations\n",
    "remove = remove.replace(\"-\", \"\")\n",
    "pattern = r\"[{}]\".format(remove)\n",
    "content = f.read().replace(\"\\n\", \"\").replace(\"\\t\",\"\").replace(\"<DOC>\",\"SSSS\")\n",
    "content = BeautifulSoup(content, \"html5lib\").get_text()\n",
    "content = re.sub(r'[0-9]+', '', content)\n",
    "content = re.sub(pattern, \"\", content).lower() \n",
    "query = content.split(\"ssss\")\n",
    "query.remove(\"\") #Query list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Inverted_Index(n_grams, collection): #create inverted index\n",
    "    gramDict = {}\n",
    "    for i in range(len(Doc_name)):\n",
    "        document = collection[i]\n",
    "        DocID = i\n",
    "        g = ngrams(document.split(),n_grams)\n",
    "        d = nltk.FreqDist(g)\n",
    "        for key in d:\n",
    "            if(key[0] in gramDict):\n",
    "                gramDict[key[0]].append([DocID,d[key]])\n",
    "            else:\n",
    "                gramDict[key[0]] = [[DocID,d[key]]]\n",
    "    return gramDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = Create_Inverted_Index(1,Collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0 \n",
    "for d in Collection:\n",
    "    content = d.split(\" \")\n",
    "    s = s + len(content)\n",
    "avgdl = s/len(Collection) # Average document length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(unigram, Collection): # tdidf vetorizer\n",
    "    bag = []\n",
    "    n_collection = len(Collection)\n",
    "    for word in unigram.keys():\n",
    "        bag.append(word)\n",
    "    v_doc = np.zeros((n_collection, len(bag)))\n",
    "    for i in range(len(bag)):\n",
    "        word = bag[i]\n",
    "        index = unigram[word]\n",
    "        idf = np.log(n_collection/len(index))\n",
    "        for ind in index:\n",
    "            did = ind[0]\n",
    "            freq = ind[1]\n",
    "            v_doc[did,i] = idf * freq/len(Collection[did].split(\" \"))\n",
    "    return v_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_doc = tfidf(unigram, Collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_search(query, v_doc, unigram, Collection): # tfidf model\n",
    "    q = []\n",
    "    bag = []\n",
    "    n_collection = len(Collection)\n",
    "    for word in unigram.keys():\n",
    "        bag.append(word)\n",
    "    v_query = np.zeros(len(bag))\n",
    "    remove = string.punctuation # remove punctuations\n",
    "    remove = remove.replace(\"-\", \"\")\n",
    "    pattern = r\"[{}]\".format(remove)\n",
    "    query =  re.sub(pattern, \"\", query) \n",
    "    for word in query.split():\n",
    "        q.append(word)\n",
    "    words =  collections.Counter(q)\n",
    "    for word in words:\n",
    "        if word in bag:\n",
    "            pos = bag.index(word)\n",
    "            index = unigram[word]\n",
    "            idf = np.log(n_collection/len(index))\n",
    "            v_query[pos] = words[word]/len(q)\n",
    "    Sim_matrix = {}\n",
    "    for i in range(n_collection):\n",
    "        Sim_matrix[i] = cosine_similarity([v_doc[i], v_query])[0,1]\n",
    "    sorted_list = sorted(Sim_matrix.items(), key=lambda kv: kv[1])\n",
    "    sorted_list.reverse() \n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Independence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_query = os.getcwd().replace(\"phase1\",\"general\")+\"\\\\test-collection\\\\cacm.rel.txt\" # load relevance information\n",
    "f = open(file_query, 'r',encoding='utf-8')\n",
    "content = f.read()\n",
    "rel = content.split(\"\\n\")\n",
    "r = []\n",
    "for line in rel:\n",
    "    l = line.split(\" \")\n",
    "    r.append(l)\n",
    "rel = pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(rel)-1):\n",
    "    while (len(rel.iloc[i,2]) < 9):\n",
    "        rel.iloc[i,2] = rel.iloc[i,2].replace(\"-\",\"-0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_index(rel,query_id): #BIM model\n",
    "    query_id = query_id\n",
    "    q = query[query_id].split(\" \")\n",
    "    if \"\" in q:\n",
    "        q.remove(\"\")\n",
    "    records = rel.loc[rel[0] == str(query_id+1)]\n",
    "    vec = np.zeros(len(q))\n",
    "    p = np.zeros(len(q))\n",
    "    s = np.zeros(len(q))\n",
    "    rel_list = []\n",
    "    for i in range(len(records)):\n",
    "        rel_doc_name = records.iloc[i,2]\n",
    "        rel_list.append(rel_doc_name)\n",
    "        rel_doc_id = Doc_name.index(rel_doc_name)\n",
    "        rel_doc = Collection[rel_doc_id]\n",
    "        for j in range(len(q)):\n",
    "            word = q[j]\n",
    "            if(word.lower() in rel_doc.lower().split(\" \")):\n",
    "                p[j] = p[j] + 1\n",
    "    nrel_list = list(set(Doc_name) - set(rel_list))\n",
    "    for i in range(len(nrel_list)):\n",
    "        nrel_doc_id = Doc_name.index(nrel_list[i])\n",
    "        nrel_doc = Collection[nrel_doc_id]\n",
    "        for j in range(len(q)):\n",
    "            word = q[j]\n",
    "            if(word.lower() in nrel_doc.lower().split(\" \")):\n",
    "                s[j] = s[j] + 1\n",
    "    score_map = {}\n",
    "    for i in range(len(Collection)):\n",
    "        score = 0\n",
    "        for j in range(len(q)):\n",
    "            word = q[j]\n",
    "            if(word.lower() in Collection[i].lower().split(\" \")):\n",
    "                pi = (p[j]+0.5)/(len(rel_list)+1)\n",
    "                si = (s[j]+0.5)/(len(nrel_list)+1)\n",
    "                score = score + np.log((pi*(1-si)) / (si*(1-pi))) \n",
    "        score_map[i] = score\n",
    "    sorted_list = sorted(score_map.items(), key=lambda kv: kv[1])\n",
    "    sorted_list.reverse() \n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1=1.2\n",
    "b=0.75\n",
    "k2=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25score(query, unigram, stopwords, collection): #BM25 model\n",
    "    score_map = {}\n",
    "    q = []\n",
    "    remove = string.punctuation # remove punctuations\n",
    "    pattern = r\"[{}]\".format(remove)\n",
    "    query =  re.sub(pattern, \"\", query) \n",
    "    for word in query.split():\n",
    "        if word not in stopwords:\n",
    "            q.append(word)\n",
    "    words =  collections.Counter(q)\n",
    "    for word in words.keys():\n",
    "        if word not in stopwords and word in unigram:\n",
    "            index = unigram[word]\n",
    "            idf = np.log((len(collection) - len(index) + 0.5)/(len(index) + 0.5)) # total document number = 1000\n",
    "            for ind in index:\n",
    "                doc_id = ind[0]\n",
    "                f = ind[1]\n",
    "                qf = words[word]\n",
    "                dl = len(collection[doc_id].split(\" \"))\n",
    "                K = k1*(1-b+b*dl/avgdl)\n",
    "                R = (f*(k1+1)/(f+K)) * ((qf*(k2+1))/(qf+k2))\n",
    "                if doc_id not in score_map:\n",
    "                    score_map[doc_id] = R*idf\n",
    "                if doc_id in score_map:\n",
    "                    score_map[doc_id] = score_map[doc_id] + R*idf   \n",
    "    sorted_list = sorted(score_map.items(), key=lambda kv: kv[1])\n",
    "    sorted_list.reverse() \n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(query_id, option, folder_name): # Task1 output\n",
    "    query_id = query_id - 1 \n",
    "    q = query[query_id]\n",
    "    model = \"\"\n",
    "    if(option == 1): #option = 1 -TFIDF\n",
    "        model = \"TFIDF\"\n",
    "        result = tfidf_search(q, v_doc, unigram, Collection)\n",
    "    elif(option == 2): #option = 2 -BIM\n",
    "        model = \"BIM\"\n",
    "        result = relevance_index(rel,query_id)\n",
    "    elif(option == 3): #option = 2 -BM25\n",
    "        model = \"BM25\"\n",
    "        result = BM25score(q, unigram, \"\", Collection)\n",
    "    else:\n",
    "        return 0\n",
    "    directory = os.getcwd() + \"\\\\Task1\\\\\" + folder_name\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    n = 0\n",
    "    txt = open(directory + \"\\\\\"+ \"Query\" + str(query_id+1) + \".txt\",'w',encoding='utf-8')\n",
    "    txt.write(\"Query:\" + q[3:] +\"\\n\")\n",
    "    for record in result:\n",
    "        n = n + 1\n",
    "        txt.write(str(query_id+1) + \", \"+ \"Q0\" + \",\"  + Doc_name[record[0]] + \",\" + str(n) + \",\" + str(record[1]) + \",\" + model +\"\\n\")\n",
    "    txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Task1_output(query):\n",
    "    n = len(query)\n",
    "    for i in range(n):\n",
    "        query_id = i + 1\n",
    "        write(query_id, 1, \"tfidf\")\n",
    "        write(query_id, 2, \"BIM\")\n",
    "        write(query_id, 3, \"BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Task1_output(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Time Stemming Using BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTS_query = [] #Do stemming query expansion\n",
    "for q in query:\n",
    "    l = q.split(\" \")\n",
    "    nl = []\n",
    "    for i in range(len(l)):\n",
    "        if(l[i] != \"\"):\n",
    "            nl.append(ps.stem(l[i]))\n",
    "    QTS_query.append(q + \" \".join(nl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = 1 #output\n",
    "for q in QTS_query:\n",
    "    result = BM25score(q, unigram, \"\", Collection)\n",
    "    directory = os.getcwd() + \"\\\\Task2\\\\\" + \"Query Time Stemming\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    n = 0\n",
    "    txt = open(directory + \"\\\\\"+ \"Query\" + str(query_id) + \".txt\",'w',encoding='utf-8')\n",
    "    txt.write(\"Query:\" + q[3:] +\"\\n\")\n",
    "    for record in result:\n",
    "        n = n + 1\n",
    "        txt.write(str(query_id) + \", \"+ \"Q0\" + \",\"  + Doc_name[record[0]] + \",\" + str(n) + \",\" + str(record[1]) + \",\" + \"BM25\" +\"\\n\")\n",
    "    txt.close()\n",
    "    query_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  pseudo relevance feedback using BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getletterbigram(string): #string to bigram\n",
    "    res_list = []\n",
    "    for i in range(len(string)-1):\n",
    "        if string[i] != \" \" and string[i+1] != \" \":\n",
    "            res_list.append(string[i:i+2])\n",
    "    return res_list      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(bi_a,bi_b): # calculate dice coef with two bigram list\n",
    "    nab = 0\n",
    "    for bi in bi_a:\n",
    "        if bi in bi_b:\n",
    "            nab += 1\n",
    "    na = len(bi_a)\n",
    "    nb = len(bi_b)\n",
    "    return 2*nab/(na+nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_search(q, unigram, stopwords, collection, k, n): #n terms be expanded, consider top k document\n",
    "    bigram_query = getletterbigram(q)\n",
    "    result = BM25score(q, unigram, stopwords, collection)\n",
    "    rel_doc_id = []\n",
    "    for i in range(k):\n",
    "        rel_doc_id.append(result[i][0])\n",
    "    dice_score_map = {} #build word and dice's coef map\n",
    "    for docid in rel_doc_id:\n",
    "        doc = collection[docid]\n",
    "        for word in doc.split(\" \"):\n",
    "            if len(word) >= 2 and word not in stopwords:\n",
    "                bi_word = getletterbigram(word)\n",
    "                dice = dice_coef(bigram_query,bi_word)\n",
    "                if word in dice_score_map:\n",
    "                    dice_score_map[word] = dice_score_map[word] + 1\n",
    "                else:\n",
    "                    dice_score_map[word] = dice\n",
    "    dice_list = sorted(dice_score_map.items(), key=lambda kv: kv[1])\n",
    "    dice_list.reverse() #get top words\n",
    "    for i in range(n):\n",
    "        q = q + \" \" + dice_list[i][0]\n",
    "    result = BM25score(q, unigram, stopwords, collection)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   what articles exist which deal with tss time sharing system anoperating system for ibm computers '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1937, 38.23662426516155),\n",
       " (2438, 35.1371901526238),\n",
       " (1656, 34.98825943048957),\n",
       " (970, 33.40191179369672),\n",
       " (1409, 33.390906295538386)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo_search(query[0], unigram, stopwords, Collection, 10,int(len(query[0].split(\" \")[3:])/3))[:5] # top 10 doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1937, 37.413449047182354),\n",
       " (2370, 33.968793919094736),\n",
       " (1070, 33.23330175450185),\n",
       " (1656, 32.161026226947335),\n",
       " (970, 31.752300934025463)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo_search(query[0], unigram, stopwords, Collection, 20,int(len(query[0].split(\" \")[3:])/3))[:5] #top 20 doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1409, 40.816321870726235),\n",
       " (1937, 37.413449047182354),\n",
       " (1656, 32.161026226947335),\n",
       " (2370, 31.947192740629326),\n",
       " (1070, 30.87741902434439)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo_search(query[0], unigram, stopwords, Collection, 5, int(len(query[0].split(\" \")[3:])/3))[:5] #top 5 doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time sharing on a computer with a small memory techniques to make time sharing attractive on a computer with a small central memory are presented small is taken to mean that only one user program plus a monitor will fit into the memory at any time the techniques depend on having two levels of secondary storage level 1 several times larger than the main memory and quite fast and level 2 many times larger and slower than level 1'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Collection[1641]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some criteria for time sharing system performance time sharing systems as defined in this article are those multiaccess systems which permit a terminal user to utilize essentially the full resources of the system while sharing its time with other terminal users it is each terminal user s ability to utilize the full resources of the system that makes quantitative evaluation of time sharing systems particularly difficult six criteria are described which have been successfully used to perform first level quantitative time sharing system performance evaluation'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Collection[1937]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = 1\n",
    "for q in query:\n",
    "    result = pseudo_search(q, unigram, stopwords, Collection, 10, int(len(q.split(\" \")[3:])/3))\n",
    "    directory = os.getcwd() + \"\\\\Task2\\\\\" + \"pseudo\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    n = 0\n",
    "    txt = open(directory + \"\\\\\"+ \"Query\" + str(query_id) + \".txt\",'w',encoding='utf-8')\n",
    "    txt.write(\"Query:\" + q[3:] +\"\\n\")\n",
    "    for record in result:\n",
    "        n = n + 1\n",
    "        txt.write(str(query_id) + \", \"+ \"Q0\" + \",\"  + Doc_name[record[0]] + \",\" + str(n) + \",\" + str(record[1]) + \",\" + \"BM25\" +\"\\n\")\n",
    "    txt.close()\n",
    "    query_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification: \n",
    "\n",
    "In Pseudo Relevant Feedback, we choose to use BM25 baseline model to find the most relevant documents. And in stemming expansion, we choose to use nltk package to do stemming processing.\n",
    "\n",
    "For Pseudo Relevant Feedback, we choose to consider top 20 documents. From the experiments we performed, we can find that if we consider k = 20, 10, 5 situations, k = 20 gives us most relevant documents. From the baseline BM25 model, the score of top 20 documents are pretty close which means top 20 documents are all pretty relevant to the query, so k = 20 is a proper decision. About n's value, we choose to use one-third the length of the query. Because if we set n with a small number the expansion is meaningless and if we set n with a large number some of the expansion term is unrelevant to the query. After experiments, we decide to let n = one-third the length of the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = 1 #add stopword processing\n",
    "for q in query:\n",
    "    result = BM25score(q, unigram, stopwords, Collection)\n",
    "    directory = os.getcwd() + \"\\\\Task3\\\\\" + \"Stopwords\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    n = 0\n",
    "    txt = open(directory + \"\\\\\"+ \"Query\" + str(query_id) + \".txt\",'w',encoding='utf-8')\n",
    "    txt.write(\"Query:\" + q[3:] +\"\\n\")\n",
    "    for record in result:\n",
    "        n = n + 1\n",
    "        txt.write(str(query_id+1) + \", \"+ \"Q0\" + \",\"  + Doc_name[record[0]] + \",\" + str(n) + \",\" + str(record[1]) + \",\" + \"BM25\" +\"\\n\")\n",
    "    txt.close()\n",
    "    query_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_query_stemming = os.getcwd().replace(\"phase1\",\"general\")+\"\\\\test-collection\\\\cacm_stem.query.txt\"\n",
    "f = open(file_query_stemming, 'r',encoding='utf-8')\n",
    "stem_query = f.read().split(\"\\n\")\n",
    "stem_query.remove(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_query_stemming = os.getcwd().replace(\"phase1\",\"general\")+\"\\\\test-collection\\\\cacm_stem.txt\"\n",
    "f = open(file_query_stemming, 'r',encoding='utf-8')\n",
    "stem_doc = f.read().split(\"#\")\n",
    "stem_doc.remove(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "docID = 0 #parsing the documents\n",
    "stem_doc_refine = []\n",
    "for line in stem_doc:\n",
    "    docID += 1\n",
    "    ID = \" \" + str(docID)\n",
    "    content = line.replace(ID,\"\").replace(\"\\n\" , \"\").replace(\"pm\",\"\")\n",
    "    content = re.sub(r'[0-9]+', '', content)\n",
    "    content = re.sub(r'\\s+', ' ', content)\n",
    "    content = content.replace(\" cacm\" + content.split(\"cacm\")[-1],\"\")\n",
    "    stem_doc_refine.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_stem = Create_Inverted_Index(1,stem_doc_refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = 1\n",
    "for q in stem_query:\n",
    "    result = BM25score(q, unigram_stem, \"\", stem_doc_refine)\n",
    "    directory = os.getcwd() + \"\\\\Task3\\\\\" + \"stem\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    n = 0\n",
    "    txt = open(directory + \"\\\\\"+ \"Query\" + str(query_id) + \".txt\",'w',encoding='utf-8')\n",
    "    txt.write(\"Query:\" + q[3:] +\"\\n\")\n",
    "    for record in result:\n",
    "        n = n + 1\n",
    "        txt.write(str(query_id+1) + \", \"+ \"Q0\" + \",\"  + Doc_name[record[0]] + \",\" + str(n) + \",\" + str(record[1]) + \",\" + \"BM25\" +\"\\n\")\n",
    "    txt.close()\n",
    "    query_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['portabl oper system ',\n",
       " 'code optim for space effici ',\n",
       " 'parallel algorithm ',\n",
       " 'distribut comput structur and algorithm ',\n",
       " 'appli stochast process ',\n",
       " 'perform evalu and model of comput system ',\n",
       " 'parallel processor in inform retriev ']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2663, 12.794266200111572),\n",
       " (2895, 12.33577807527525),\n",
       " (140, 12.316624827868388),\n",
       " (1261, 12.120954345937298),\n",
       " (1794, 12.029678522888092),\n",
       " (391, 12.029678522888092),\n",
       " (2684, 11.702198529453094),\n",
       " (2951, 11.49411091376892),\n",
       " (2699, 11.342224859330392),\n",
       " (1301, 11.004198226316099)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BM25score(stem_query[2], unigram_stem, \"\", stem_doc_refine)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2211, 11.605824480517246),\n",
       " (2031, 11.103369627008234),\n",
       " (861, 11.058678015946347),\n",
       " (2577, 10.804005315968805),\n",
       " (1662, 10.681017578139294),\n",
       " (87, 9.998133177724037),\n",
       " (51, 9.892719009707449),\n",
       " (2974, 9.024335841506062),\n",
       " (2913, 8.802269732969503),\n",
       " (1950, 8.230148164441331)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BM25score(stem_query[3], unigram_stem, \"\", stem_doc_refine)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1810, 18.201511228789993),\n",
       " (3074, 15.559218402502596),\n",
       " (1612, 15.515972961019969),\n",
       " (2663, 15.224463101237347),\n",
       " (3155, 14.469925600356914),\n",
       " (633, 14.380818990483315),\n",
       " (2277, 14.272644502938494),\n",
       " (656, 13.835675383104086),\n",
       " (890, 13.779847199315435),\n",
       " (291, 13.674507206612034)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BM25score(stem_query[6], unigram_stem, \"\", stem_doc_refine)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parallelism in tape sorting two methods for employing parallelism in tape sorting are presented method a is the natural way to use parallelism method b is new both approximately achieve the goal of reducing the processing time by a divisor which is the number of processors'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Collection[2663]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on foster s information storage and retrieval using avl trees'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Collection[2277]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'procedure oriented language statements to facilitate parallel processing two statements are suggested which allow a programmer writing in a procedure oriented language to indicate sections of program which are to be executed in parallel the statements are do together and hold these serve partly as brackets in establishing a range of parallel operation and partly to define each parallel path within this range do togethers may be nested the statements should be particularly effective for use with computing devices capable of attaining some degree of compute compute overlap'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Collection[1261]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the experiment we performed, the result is satisfying and pretty relevant to those queries.\n",
    "\n",
    "I selected three queries which are relevant in some way, and we can find some similarities in the result. In document CACM-2664 (whose id in my program is 2663) contains both \"parallel\", \"process\" and it was ranked in both queries (\"parallel algorithm\", \"parallel processor in inform retriev\")."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Zhan\n",
      "[nltk_data]     Kefei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue, Empty\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import string\n",
    "k1=1.2\n",
    "b=0.75\n",
    "k2=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = os.getcwd().replace(\"extra\\\\code\",\"general\")+\"\\\\test-collection\\\\common_words\" #load stopwords\n",
    "f = open(stops, 'r',encoding='utf-8')\n",
    "stopwords = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Collection = [] # load documents and do parsing job\n",
    "Doc_name = []\n",
    "folder_d = os.getcwd().replace(\"extra\\\\code\",\"general\")+\"\\\\test-collection\\\\cacm\"\n",
    "for filename in os.listdir(folder_d):\n",
    "    Doc_name.append(filename.replace(\".html\",\"\"))\n",
    "    file_d = folder_d + \"\\\\\" + filename\n",
    "    f = open(file_d, 'r',encoding='utf-8')\n",
    "    remove = string.punctuation # remove punctuations\n",
    "    pattern = r\"[{}]\".format(remove)\n",
    "    content = f.read().replace(\"\\n\",\" \").replace(\"\\t\",\" \") \n",
    "    content = BeautifulSoup(content, \"html5lib\").get_text()\n",
    "    content = re.sub(pattern, \" \", content).lower() \n",
    "    content = content.replace(\" cacm\" + content.split(\"cacm\")[-1],\"\")  \n",
    "    #ignore the digits that commonly appear in the end of the documentsâ€™ content.\n",
    "    Collection.append(re.sub(r'\\s+', ' ', content).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_query = os.getcwd().replace(\"extra\\\\code\",\"general\")+\"\\\\test-collection\\\\cacm.query.txt\" #load query list\n",
    "f = open(file_query, 'r',encoding='utf-8')\n",
    "remove = string.punctuation # remove punctuations\n",
    "remove = remove.replace(\"-\", \"\")\n",
    "pattern = r\"[{}]\".format(remove)\n",
    "content = f.read().replace(\"\\n\", \"\").replace(\"\\t\",\"\").replace(\"<DOC>\",\"SSSS\")\n",
    "content = BeautifulSoup(content, \"html5lib\").get_text()\n",
    "content = re.sub(r'[0-9]+', '', content)\n",
    "content = re.sub(pattern, \"\", content).lower() \n",
    "query = content.split(\"ssss\")\n",
    "query.remove(\"\") #Query list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Inverted_Index(n_grams, collection): #create inverted index\n",
    "    gramDict = {}\n",
    "    for i in range(len(Doc_name)):\n",
    "        document = collection[i]\n",
    "        DocID = i\n",
    "        g = ngrams(document.split(),n_grams)\n",
    "        d = nltk.FreqDist(g)\n",
    "        for key in d:\n",
    "            if(key[0] in gramDict):\n",
    "                gramDict[key[0]].append([DocID,d[key]])\n",
    "            else:\n",
    "                gramDict[key[0]] = [[DocID,d[key]]]\n",
    "    return gramDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = Create_Inverted_Index(1,Collection)\n",
    "s = 0 \n",
    "for d in Collection:\n",
    "    content = d.split(\" \")\n",
    "    s = s + len(content)\n",
    "avgdl = s/len(Collection) # Average document length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25score(query, unigram, stopwords, collection): #BM25 model\n",
    "    score_map = {}\n",
    "    q = []\n",
    "    remove = string.punctuation # remove punctuations\n",
    "    pattern = r\"[{}]\".format(remove)\n",
    "    query =  re.sub(pattern, \"\", query) \n",
    "    for word in query.split():\n",
    "        if word not in stopwords:\n",
    "            q.append(word)\n",
    "    words =  collections.Counter(q)\n",
    "    for word in words.keys():\n",
    "        if word not in stopwords and word in unigram:\n",
    "            index = unigram[word]\n",
    "            idf = np.log((len(collection) - len(index) + 0.5)/(len(index) + 0.5)) # total document number = 1000\n",
    "            for ind in index:\n",
    "                doc_id = ind[0]\n",
    "                f = ind[1]\n",
    "                qf = words[word]\n",
    "                dl = len(collection[doc_id].split(\" \"))\n",
    "                K = k1*(1-b+b*dl/avgdl)\n",
    "                R = (f*(k1+1)/(f+K)) * ((qf*(k2+1))/(qf+k2))\n",
    "                if doc_id not in score_map:\n",
    "                    score_map[doc_id] = R*idf\n",
    "                if doc_id in score_map:\n",
    "                    score_map[doc_id] = score_map[doc_id] + R*idf   \n",
    "    sorted_list = sorted(score_map.items(), key=lambda kv: kv[1])\n",
    "    sorted_list.reverse() \n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(bi_a,bi_b): # calculate dice coef with two bigram list\n",
    "    nab = 0\n",
    "    for bi in bi_a:\n",
    "        if bi in bi_b:\n",
    "            nab += 1\n",
    "    na = len(bi_a)\n",
    "    nb = len(bi_b)\n",
    "    return 2*nab/(na+nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getletterbigram(string): #string to bigram\n",
    "    res_list = []\n",
    "    for i in range(len(string)-1):\n",
    "        if string[i] != \" \" and string[i+1] != \" \":\n",
    "            res_list.append(string[i:i+2])\n",
    "    return res_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prf_expansion(q, unigram, stopwords, collection, k, n):\n",
    "    bigram_query = getletterbigram(q)\n",
    "    result = BM25score(q, unigram, stopwords, collection)\n",
    "    rel_doc_id = []\n",
    "    for i in range(k):\n",
    "        rel_doc_id.append(result[i][0])\n",
    "    dice_score_map = {} #build word and dice's coef map\n",
    "    for docid in rel_doc_id:\n",
    "        doc = collection[docid]\n",
    "        for word in doc.split(\" \"):\n",
    "            if len(word) >= 2 and word not in stopwords:\n",
    "                bi_word = getletterbigram(word)\n",
    "                dice = dice_coef(bigram_query,bi_word)\n",
    "                if word in dice_score_map:\n",
    "                    dice_score_map[word] = dice_score_map[word] + 1\n",
    "                else:\n",
    "                    dice_score_map[word] = dice\n",
    "    dice_list = sorted(dice_score_map.items(), key=lambda kv: kv[1])\n",
    "    dice_list.reverse() #get top words\n",
    "    for i in range(n):\n",
    "        q = q + \" \" + dice_list[i][0]\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pwq(word,q,bucket,C):\n",
    "    pd = 1/len(C)\n",
    "    res = 0\n",
    "    for doc in C:\n",
    "        temp = pd\n",
    "        n_doc = len(doc.split(\" \"))\n",
    "        wordmap = collections.Counter(doc.split(\" \"))\n",
    "        if word in wordmap:\n",
    "            pwd = wordmap[word]/n_doc\n",
    "        else:\n",
    "            pwd = 1/(avgdl * 10)\n",
    "        for qword in q.split(\" \"):\n",
    "            if qword not in stopwords:\n",
    "                if qword in wordmap:\n",
    "                    pq = wordmap[qword]/n_doc\n",
    "                else:\n",
    "                    pq = 1/(avgdl * 10)\n",
    "                temp = temp * pq\n",
    "        temp = temp * pwd\n",
    "        res = res + temp\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pf_model(q, unigram, stopwords, collection, k): #pseudo relevance feedback based on kl divergence\n",
    "    result = BM25score(q, unigram, stopwords, collection)[:k] # select top k documents as relevant ones\n",
    "    exq = prf_expansion(q, unigram, stopwords, collection, 10, int(len(q.split(\" \"))/3)) #query expansion\n",
    "    bucket = []\n",
    "    C = []\n",
    "    i = 0\n",
    "    for record in result:\n",
    "        i = i + 1\n",
    "        words = collection[record[0]].split(\" \")\n",
    "        C.append(collection[record[0]])\n",
    "        for word in words:\n",
    "            if word not in stopwords and word not in bucket:\n",
    "                bucket.append(word)\n",
    "    model = {}\n",
    "    total = 0\n",
    "    for word in bucket:\n",
    "        model[word] = pwq(word,q,bucket,C)\n",
    "    for word in model:\n",
    "        total += model[word]\n",
    "    for word in model:\n",
    "        model[word] = model[word]/total\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_rank(q, unigram, stopwords, collection, k):\n",
    "    rel_model = pf_model(q, unigram, stopwords, collection, k)\n",
    "    score_map = {}\n",
    "    docid = 0\n",
    "    for doc in collection:\n",
    "        kl_score = 0\n",
    "        wordmap = collections.Counter(doc.split(\" \"))\n",
    "        l = len(doc.split(\" \"))\n",
    "        for word in doc.split(\" \"):\n",
    "            if word in rel_model:\n",
    "                if word in wordmap:\n",
    "                    kl_score = kl_score + rel_model[word] * np.log(wordmap[word]/l)\n",
    "        score_map[docid] = kl_score\n",
    "        docid = docid + 1\n",
    "    rlist = sorted(score_map.items(), key=lambda kv: kv[1])\n",
    "    return rlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = 1\n",
    "for q in query:\n",
    "    result = kl_rank(q, unigram, stopwords, Collection, 10)\n",
    "    directory = os.getcwd().replace(\"code\",\"result\")\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    n = 0\n",
    "    txt = open(directory + \"\\\\\"+ \"Query\" + str(query_id) + \".txt\",'w',encoding='utf-8')\n",
    "    txt.write(\"Query:\" + q[3:] +\"\\n\")\n",
    "    for record in result[:100]:\n",
    "        n = n + 1\n",
    "        txt.write(str(query_id) + \", \"+ \"Q0\" + \",\"  + Doc_name[record[0]] + \",\" + str(n) + \",\" + str(record[1]) + \",\" + \"BM25\" +\"\\n\")\n",
    "    txt.close()\n",
    "    query_id += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
